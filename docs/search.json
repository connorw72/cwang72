[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Connor Wang",
    "section": "",
    "text": "Hello! My name is Connor. I am a sophomore at Pomona College pursuing a Computer Science degree with a minor in data science. Outside of school I am on the Pomona-Pitzer football, where we compete in the SCIAC. My hobbies include going to the gym, playing video games, and spending time with friends."
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "Arrests and Warnings categorized by race across the states of Arizona, Maryland, and Colorado",
    "section": "",
    "text": "R Packages\nlibrary(ggplot2)\nEstablish Database Connection\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)"
  },
  {
    "objectID": "project5.html#introduction",
    "href": "project5.html#introduction",
    "title": "Arrests and Warnings categorized by race across the states of Arizona, Maryland, and Colorado",
    "section": "Introduction",
    "text": "Introduction\nWhen a traffic stop is made, a cop can choose to arrest an individual, write them a citation, or give them a warning. A warning is the most minor, while a citation tends to mean a fine and other possible punishments, with an arrest being the worst as there is more reason to believe that a true crime has been committed. Using the Stanford Open Policing Project, I wanted to explore data regarding the amount of warnings, citations, and arrests made based on race in various states. I thought this was interesting and important to look at to see if there are any trends based on the severity of the punishment given to an individual and their race. Given the diversity of the United States of America, I chose to use 3 states that represented different areas of the country - the west, through Arizona, the midwest through Colorado, and the east coast, through Maryland. Although they have different populations, with Arizona over a million people more than the next one, Maryland, I thought it was best to get a good representation of different regions of the country. I split the data up based on the punishment given to the person who was stopped, which I then used to create a bar graphs with side by side comparisons split by race and state.\n\nSELECT 'Maryland' AS state, subject_race, COUNT(*) AS num_arrests\nFROM md_statewide_2020_04_01\nWHERE arrest_made = 1\nGROUP BY subject_race \nHAVING num_arrests &gt; 300\n\nUNION\n\nSELECT 'Arizona' AS state, subject_race, COUNT(*) AS num_arrests\nFROM az_statewide_2020_04_01\nWHERE arrest_made = 1\nGROUP BY subject_race \nHAVING num_arrests &gt; 300\n\nUNION\n\nSELECT 'Colorado' AS state, subject_race, COUNT(*) AS num_arrests\nFROM co_statewide_2020_04_01\nWHERE arrest_made = 1\nGROUP BY subject_race \nHAVING num_arrests &gt; 300\n\nORDER BY state, num_arrests DESC\n\nThe SQL query creates a table with information regarding the number of arrests made in Maryland, Colorado, and Arizona, categorized by race. The information is limited to data points over 300 arrests made as anything lower was not a significant piece of information. It is compiled into one table, where it displays each state, with the top data point representing the race that had the most arrests made in that state. It is then put into the table “arrests_table” which is used to create the visualization of the arrests made.\n\nggplot(arrests_table, aes(x = subject_race, y = num_arrests, fill = state)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Number of arrests by race across the states \\n\n    Arizona, Colorado, and Maryland\",\n    x = \"Race\",\n    y = \"Number of Arrests\"\n  )\n\n\n\n\n\n\n\n\nThe bar graph represents the number of arrests made by race, with each race having three bars representative of the different states. Arizona has the most arrests made for every race. Although this can be attributed to its larger population, the amount of arrests made are also much larger than the other states. This could point to a possibility of more crime in Arizona than Colorado and Maryland, as the number of arrests are not proportional to the total population of the state. Maryland had similar amounts of arrests made for both black and white people, while the other states had much more arrests made on white people than made on black people. The amount of white people is the highest race arrested for each state, which is likely due to population makeup. Overall, there does not seem to be any alarming trends in the arrests made by state.\n\nSELECT 'Maryland' AS state, subject_race, COUNT(*) AS num_citations\nFROM md_statewide_2020_04_01\nWHERE citation_issued = 1\nGROUP BY subject_race \nHAVING num_citations &gt; 1000\n\nUNION\n\nSELECT 'Arizona' AS state, subject_race, COUNT(*) AS num_citations\nFROM az_statewide_2020_04_01\nWHERE citation_issued = 1\nGROUP BY subject_race \nHAVING num_citations &gt; 1000\n\nUNION\n\nSELECT 'Colorado' AS state, subject_race, COUNT(*) AS num_citations\nFROM co_statewide_2020_04_01\nWHERE citation_issued = 1\nGROUP BY subject_race \nHAVING num_citations &gt; 1000\n\nORDER BY state, num_citations DESC\n\nThe SQL query creates a table with information regarding the number of citations given in Maryland, Colorado, and Arizona, categorized by race. The information is limited to data points over 1000 citations given as anything lower was not a significant piece of information. It is compiled into one table, where it displays each state, with the top data point representing the race that had the most citations given in that state. It is then put into the table “citations_table” which is used to create the visualization of the citations given.\n\nggplot(citations_table, aes(x = subject_race, y = num_citations, fill = state)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Number of citations given by race across the states \\n \n    Maryland, Arizona, and Colorado\",\n    x = \"Race\",\n    y = \"Number of Citations Issued\"\n  )\n\n\n\n\n\n\n\n\nThe bar graph represents the number of citations made by race, with each race having three bars to represent the different states. Arizona has an overwhelmingly high amount of citations given to white and hispanic people, while Maryland gave the most citations to black people. A clear difference is seen in Maryland and Arizona, where the numbers of citations given to white people and black people are similar in Maryland, while much more white people were given citations than black people in Arizona. The difference between hispanic and white in Arizona is quite substantial as well, with the number of citations written to hispanic people about half that of white people. Based on our data, we can see that it is contant across the states for white people to be written the most citations.\n\nSELECT 'Maryland' AS state, subject_race, COUNT(*) AS num_warnings\nFROM md_statewide_2020_04_01\nWHERE warning_issued = 1\nGROUP BY subject_race \nHAVING num_warnings &gt; 5000 \n\nUNION\n\nSELECT 'Arizona' AS state, subject_race, COUNT(*) AS num_warnings\nFROM az_statewide_2020_04_01\nWHERE warning_issued = 1\nGROUP BY subject_race \nHAVING num_warnings &gt; 5000 \n\nUNION \n\nSELECT 'Colorado' AS state, subject_race, COUNT(*) AS num_warnings\nFROM co_statewide_2020_04_01\nWHERE warning_issued = 1\nGROUP BY subject_race \nHAVING num_warnings &gt; 5000 \n\nORDER BY state, num_warnings DESC\n\nThe SQL query creates a table with information regarding the number of warnings given in Maryland, Colorado, and Arizona, categorized by race. The information is limited to data points over 5000 warnings given as anything lower was not a significant piece of information. It is compiled into one table, where it displays each state, with the top data point representing the race that had the most warnings given in that state. It is then put into the table “warnings_table” which is used to create the visualization of the warnings given.\n\nggplot(warnings_table, aes(x = subject_race, y = num_warnings, fill = state)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Number of warnings given by race across the states \\n\n    Arizona, Colorado, and Maryland\",\n    x = \"State\",\n    y = \"Number of Warnings issued\"\n  )\n\n\n\n\n\n\n\n\nThe bar graph represents the number of warnings issued by race, with each race having three bars to separate data amongst the states. Likely as a result of population differences, white people have the highest amount of warnings issued to them across all states. The second highest for the state of Maryland is black people, while the second highest for Arizona is Hispanic. In Colorado, the second is Hispanic however it is much less than that of white people. It could be a reflection of the proportion of the population, but the difference in citations for black people and white people is substantial in Arizona and Colorado, however not as far off in Maryland."
  },
  {
    "objectID": "project5.html#conclusion",
    "href": "project5.html#conclusion",
    "title": "Arrests and Warnings categorized by race across the states of Arizona, Maryland, and Colorado",
    "section": "Conclusion",
    "text": "Conclusion\nIn order to create our visualizations of arrests made, citations issued, and warnings given by race across the states of Arizona, Maryland, and Colorado in traffic stops, I utilized SQL queries. Each dataset had a variable pertaining to arrests, citations, or warnings. I extracted number of each that was given, grouping by race in order to give the total number of each category given by race. I then used union to combine data and create tables for each type of punishment given. I followed the same steps for each arrests, citations, and warnings, where the variable was given a value of 1 if it was the outcome, and 0 if it was not, so 1 was seen as a datapoint that I wanted to track. I then removed the lowest numbers by race, as they were often just the “other” or “unknown” category, so the data was not extremely useful. From these combined tables I then created the graphs to visualize the data and look for any clear trends."
  },
  {
    "objectID": "project5.html#references",
    "href": "project5.html#references",
    "title": "Arrests and Warnings categorized by race across the states of Arizona, Maryland, and Colorado",
    "section": "References",
    "text": "References\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "dataviz/horrormovies.html",
    "href": "dataviz/horrormovies.html",
    "title": "Horror Movies",
    "section": "",
    "text": "Based on our scatterplot, higher budgets do not always lead to higher revenues. The highest revenue came from an average budget."
  },
  {
    "objectID": "dataviz/horrormovies.html#references",
    "href": "dataviz/horrormovies.html#references",
    "title": "Horror Movies",
    "section": "References",
    "text": "References\nThe Movie Database (TMDB), www.themoviedb.org/. Accessed 15 Feb. 2025.\nRfordatascience. “Tidytuesday/Data/2022/2022-11-01 at Main · Rfordatascience/Tidytuesday.” GitHub, github.com/rfordatascience/tidytuesday/tree/main/data/2022/2022-11-01. Accessed 15 Feb. 2025."
  },
  {
    "objectID": "dataviz/bobsburgers.html",
    "href": "dataviz/bobsburgers.html",
    "title": "Bobs Burgers",
    "section": "",
    "text": "Based on the scatterplot above, there is no clear correlation between amount of questions and exclamations."
  },
  {
    "objectID": "dataviz/bobsburgers.html#references",
    "href": "dataviz/bobsburgers.html#references",
    "title": "Bobs Burgers",
    "section": "References",
    "text": "References\nPoncest. “Poncest/Bobsburgersr: Collection of Datasets from the Bob’s Burgers Animated Sitcom.” GitHub, github.com/poncest/bobsburgersR. Accessed 15 Feb. 2025.\nRfordatascience. “Tidytuesday/Data/2024/2024-11-19/Readme.Md at Main · Rfordatascience/Tidytuesday.” GitHub, github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-11-19/readme.md. Accessed 15 Feb. 2025."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Exploring Bias in an Amazon Hiring AI",
    "section": "",
    "text": "Back in 2014 a team of Amazon engineers developed an experimental hiring tool that used artificial intelligence to help sort through job applications. The model was meant to increase efficiency by automatically scoring resumes without the need for a human to spend the time reading through them. Candidates were given scores from one to five stars, in order to highlight the most promising candidates for recruiters to look at (Reuters). The team utilized resumes from the past ten years submitted to the company to train the model. The algorithm then learned patterns from this data to predict strong candidates, based on past hirings. Issues arose when the company noticed that the ratings were systematically discriminating against women when they applied for technical jobs (ACLU). Given that the program used past Amazon resumes as blueprints for what an ideal resume looks like, and that males dominated the tech industry at the time, the algorithm learned to favor male applicants. The male candidates were preferred due to various key words such as “captured”, which were verbs that were more common for male engineers to use. Women candidates were penalized for mentioning the word “women’s” in relation to clubs or teams, and discriminated against all-woman colleges (ACLU). As a result of this bias, Amazon stopped using this particular software program and moved onto a recruiting engine that would help with more “rudimentary chores” (Reuters). The overall idea of the algorithm made sense - take in lots of data, and use it to train the model to look out for what Amazon has considered a successful candidate in the past. When doing so, it takes out the human aspect of looking through resumes and getting a feel for the person submitting the job application. The model was just going off the data it was given, which happened to be predominantly male resumes."
  },
  {
    "objectID": "project4.html#the-scenario",
    "href": "project4.html#the-scenario",
    "title": "Exploring Bias in an Amazon Hiring AI",
    "section": "",
    "text": "Back in 2014 a team of Amazon engineers developed an experimental hiring tool that used artificial intelligence to help sort through job applications. The model was meant to increase efficiency by automatically scoring resumes without the need for a human to spend the time reading through them. Candidates were given scores from one to five stars, in order to highlight the most promising candidates for recruiters to look at (Reuters). The team utilized resumes from the past ten years submitted to the company to train the model. The algorithm then learned patterns from this data to predict strong candidates, based on past hirings. Issues arose when the company noticed that the ratings were systematically discriminating against women when they applied for technical jobs (ACLU). Given that the program used past Amazon resumes as blueprints for what an ideal resume looks like, and that males dominated the tech industry at the time, the algorithm learned to favor male applicants. The male candidates were preferred due to various key words such as “captured”, which were verbs that were more common for male engineers to use. Women candidates were penalized for mentioning the word “women’s” in relation to clubs or teams, and discriminated against all-woman colleges (ACLU). As a result of this bias, Amazon stopped using this particular software program and moved onto a recruiting engine that would help with more “rudimentary chores” (Reuters). The overall idea of the algorithm made sense - take in lots of data, and use it to train the model to look out for what Amazon has considered a successful candidate in the past. When doing so, it takes out the human aspect of looking through resumes and getting a feel for the person submitting the job application. The model was just going off the data it was given, which happened to be predominantly male resumes."
  },
  {
    "objectID": "project4.html#consent",
    "href": "project4.html#consent",
    "title": "Exploring Bias in an Amazon Hiring AI",
    "section": "Consent",
    "text": "Consent\nWhen using participant data for research or training a model, it is important that they consent to their data being used. Since data can include personal information that participants may want to keep private, it is vital that they know where their data will be going and how it will be used. They must allow for this use of their data, or state they are fine with how the company intends to use the data. In this case, given that resumes from ten years back were used, it is highly unlikely that job applicants signed on to have their resumes included as data when attempting to train this hiring model (ACLU). The individuals submitted the job applications expecting a human recruiter to read and evaluate their resume, not to be used for a machine learning experiment. There was no informed consent from these individuals which raises ethical concerns because the applicants are unaware of the context in which their data is being used. Although they were willingly submitting their personal information in the form of job applications and resumes, we do not know if they ever meaningfully said that it was okay to use their data to train the model. Thus, it is difficult to get consent for applications data that are unforeseen."
  },
  {
    "objectID": "project4.html#unintended-use",
    "href": "project4.html#unintended-use",
    "title": "Exploring Bias in an Amazon Hiring AI",
    "section": "Unintended Use",
    "text": "Unintended Use\nOftentimes data can be used in ways that the user did not originally intend for. In the example of the Amazon model, data was used that the user submitted for the purpose of getting hired - not to train a machine learning model (Reuters). The primary issue with unintended use is that users will submit data for one purpose, but it ends up being used in another. In this case, similar to the aforementioned consent case of unforeseen applications, the repurposing of this data that was not originally meant to be used for this model ended up having unintended consequences. It is necessary to ensure that data being used for a different circumstance than originally intended is valid and can be used in this context. Since the data was just for various Amazon job applications, the model recognized the past hiring patterns, which happened to be in a male-dominated field. The applicants and engineers did not mean for these harms, yet they occurred because the data was used in a way that it was not supposed to be."
  },
  {
    "objectID": "project4.html#generalization-of-the-data",
    "href": "project4.html#generalization-of-the-data",
    "title": "Exploring Bias in an Amazon Hiring AI",
    "section": "Generalization of the Data",
    "text": "Generalization of the Data\nWhen analyzing data it is important to keep in mind what we are analyzing it for. The data that we are using should be representative of the people who we will eventually be using the algorithm on to make sure it accurately makes predictions for the representative population. Otherwise, we could be using data that is completely unrelated or missing a part of the representative population, and thus creating an algorithm that does not correctly apply. Using a dataset of Amazon’s previous tech industry hiring history meant one that consisted of predominantly male resumes (Reuters). This created training data for the model that was not representative of the population, which in this case was Amazon’s applicant pool for tech related jobs. Based on data that was not generalizable to the whole applicant pool, the model was flawed with gender biases coming from patterns or language that is found in male resumes. The model was going off the data it had, creating a lack of accuracy when it came to gender-diverse applicants. Attempting to make generalizations off of a non-representative dataset can dangerous when attempting to build models that make big decisions, such as who to hire."
  },
  {
    "objectID": "project4.html#identifiable-data",
    "href": "project4.html#identifiable-data",
    "title": "Exploring Bias in an Amazon Hiring AI",
    "section": "Identifiable Data",
    "text": "Identifiable Data\nWhen working with personal data it is very important to ensure that people are fine with their data being out there and used in ways that they might not actively know about. An easy way to be free of this ethical concern is to make sure the data is anonymous or old. This way, there are no privacy concerns as there is no way to connect the information in the data to specific people. In the case of the Amazon model, resumes are extremely identifiable documents. Although resumes differ from person to person, it will likely include information such as a full name, address, contact information, work history, and schools. All of these are ways to easily create links to individuals. While it is possible to make resumes somewhat anonymous and remove the direct links to people, there is no indication that Amazon did this with the resumes used to train the hiring algorithm. On top of this, the data being within 10 years does not make it old enough to fight ethical arguments against its use (ACLU). The data was likely stored and saved in development or logs which means a ton of personal data that is not exactly being stored safely. When data can store identifiers to an individual it is hard to protect the identity of the person.\n\nReferences\nInsight - Amazon Scraps Secret AI Recruiting Tool That Showed Bias against Women | Reuters, www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/. Accessed 15 Apr. 2025.\n“Why Amazon’s Automated Hiring Tool Discriminated against Women: ACLU.” American Civil Liberties Union, 27 Feb. 2023, www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Netflix Data Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\n\nDataset\nThe Dataset that I have chosen to explore is about Netflix shows. Netflix is a streaming service which includes various documentaries, TV shows, movies, and more. The data consists of TV shows and movies that were available on Netflix from 2019 to 2021. The data was collected through Flixable, a third-party Netflix search engine. The data includes the type of picture it is, such as movie or TV show, the title, the director, the cast, the country it originates from, the date it was added to netflix, the year it was released, the ratings(TV-MA, PG-13, etc.), the duration, what categories it is listed in, and the description of the picture. Through this information I have created various questions of interest that come with accompanying visuals in order to share my findings.\n\nnetflix_titles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv')\n\nnetflix_titles\n\n\n\nQuestion of Interest 1\nThe first question of interest that I came up with for this dataset is how has movie duration changed over time? Since the data about what is streaming on Netflix included the duration of movies in minutes as well as the year they were released, I decided it would be interesting to note how this has changed over time. Going into it I assumed that movies have gotten longer over time. Movies that come out these days seem to always be two plus hours, often being two and a half hours or even three on some of the extreme cases. Although not all movies are included on Netflix, it would still give a good idea of how movies have trended in terms of duration over time. I found the opposite to be true. Based on the linear regression model I created below there is a clear downwards trend over time. Movies have in fact gotten shorter in duration throughout the years and. Save for some outliers, the general trend is the same with newer movies becoming shorter and shorter over time.\n\nmovies_only &lt;- netflix_titles %&gt;%\n  filter(type == \"Movie\") %&gt;%\n  mutate(movie_duration = as.numeric(str_extract(duration, \"\\\\d+\"))) %&gt;%\n  filter(!is.na(duration))\n\nggplot(movies_only, aes(x = release_year, y = movie_duration)) + \n  geom_point(color = \"lightsalmon3\") +\n  geom_smooth(method = \"lm\", color = \"steelblue3\", se = FALSE) + \n  labs(\n    title = \"Has Movie duration increased over time?\",\n    x = \"Year Released\",\n    y = \"Duration of Movie in Minutes\"\n  )\n\n\n\n\n\n\n\n\nPreviously mentioned in the description of the question of interest, this plot depicts the release year of a movie on the x-axis versus the duration of the movie in minutes on the y-axis. This creates a comparison between movie duration over time. You see a negatively-sloped linear regression line, indicating that movie duration time has decreased in general over the years. Furthermore, more movies in general have come out as time has progressed which could be a part of it. There are more shorter movies out there as they are likely easier to produce than longer ones.\n\n\nQuestion of Interest 2\nThe second question of interest that I came up with was which countries have the most TV Shows and Movies that are about the future. I wondered if there would be a connection to depictions of the future with countries that tend to be seen as more technologically advanced or first world. I explored the descriptions that included the word future in them and used this to determine if the picture had anything to do with the future. What I found generally aligned with what I thought as the United States had the highest amount of TV shows and movies dealing with ‘future’ ideas. This is also likely why I thought about this because I am exposed to so many shows and movies that align with this, having grown up watching Netflix in the states. The bar chart shows a large discrepancy from the United States to the next country, India. This uses only the top 10 countries that have future in descriptions.\n\ncontent_about_future &lt;- netflix_titles %&gt;%\n  filter(str_detect(description, \"(?i)future\")) %&gt;%\n  filter(!is.na(country)) %&gt;%\n  group_by(country) %&gt;%\n  summarise(count = sum(n())) %&gt;%\n  arrange(desc(count)) %&gt;%\n  slice_head(n = 10)\n\nggplot(content_about_future, aes(x = country, y = count)) +\n  geom_col(fill = \"plum3\") + \n  labs(\n    title = \"Countries with TV Shows and Movies about the 'future'\",\n    x = \"Country\",\n    y = \"Number of TV Shows/Movies\"\n  ) +\n  theme(axis.text.x = element_text(size = 7))\n\n\n\n\n\n\n\n\nPreviously mentioned in the description of the question of interest, this plot shows the top 10 countries that have TV Shows or Movies on Netflix with future in their description. The y-axis shows how many TV Shows or Movies follow this criteria. Outside of the United States, there are not very many TV Shows or Movies that are about future scenarios. The US is the clear leader by about 20 projects. The next is India with about 8 or 9. Most have below 5.\n\n\nQuestion of Interest 3\nThe third question of interest that I came up with was which words come before the word ‘of’ the most in movie titles. I thought about this because there seem to be many shows and movies that use of, such as Pirates of the Caribbean, Wizard of Oz, and many more. I found this would be an interesting thing to see. I did not have much prediction going in, but after seeing the data it makes sense. Words such as story and legend tend to come before of. Many title creators likely see it as a good way of making a fantastical-sounding title, such as the story of ___ or the legend of ___. I used the title column in the data to create a lookaround that checked for ’ of’ following the word. This is how I got the 10 most used words before ‘of’, finding an interesting trend that I do not think many people would tend to think of.\n\nwords &lt;- netflix_titles %&gt;%\n  mutate(of = str_extract(str_to_lower(title), \"\\\\b\\\\w+(?= of)\")) %&gt;%\n  filter(!is.na(of)) %&gt;%\n  group_by(of) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice_head(n = 10)\n\nggplot(words, aes(x = of, y = n)) +\n  geom_col(aes(fill = of)) + \n  labs(\n    title = \"10 Most common words that precede 'of' in TV show/movie titles\",\n    x = \"Word before 'of'\",\n    y = \"Number of titles that it appears before 'of' in\"\n  )\n\n\n\n\n\n\n\n\nPreviously mentioned in the question of interest, this plot shows the top 10 words that come before ‘of’ in the titles of TV shows or movies. The words legend and story are both tied for first place the most amount of times it comes before the word ‘of’ in a title. The 7 other words also appear in front of ‘of’ a considerable amount of times, all of which having at least 6 projects in which they are used in the title prior to the use of ‘of’. The different words are labeled by both the x-axis and the legend. Each color corresponds to a different word that precedes ‘of’.\n\n\nReferences\nBansal, Shivam. “Netflix Movies and TV Shows.” Kaggle, 27 Sept. 2021, www.kaggle.com/datasets/shivamb/netflix-shows.\nRfordatascience. “Tidytuesday/Data/2021/2021-04-20/Readme.Md at Main · Rfordatascience/Tidytuesday.” GitHub, github.com/rfordatascience/tidytuesday/blob/main/data/2021/2021-04-20/readme.md. Accessed 2 Mar. 2025."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Fast Food",
    "section": "",
    "text": "library(readr)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nfastfood &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-09-04/fastfood_calories.csv\")"
  },
  {
    "objectID": "project3.html#plan",
    "href": "project3.html#plan",
    "title": "Fast Food",
    "section": "Plan",
    "text": "Plan\nFor the small simulation study I will develop a lineup protocol for visual inference to investigate there is a statistically significant connection between calorie and sodium content in fast food items across various restaurants. In order to do this I have found a TidyTuesday dataset that includes nutritional information sourced from fastfoodnutrition.org. In order to test this possible relation I will use plots to determine if there is a connection that is more extreme than what would be expected under a null hypothesis of no correlation between calorie content and sodium content. If the representative plot is recognizable amongst many null plots, then the relationship is not due to pure chance and we have visual evidence to prove this. I chose to explore this relationship due to the importance of health and understanding what you are eating. Calories and sodium are two of the most prominent factors when it comes to eating healthier, so I wanted to find out if they also correlated with one another.\n\ncalories_sodium &lt;- fastfood %&gt;%\n  filter(!is.na(calories), !is.na(sodium)) %&gt;%\n  select(item, restaurant, calories, sodium)\n\nnull &lt;- function(data) {\n  data %&gt;%\n    mutate(sodium = sample(sodium))\n}\n\nnull_data &lt;- map(1:19, ~null(calories_sodium) %&gt;%\n                   mutate(plot_id = as.character(.x)))\n\ntrue_data &lt;- calories_sodium %&gt;%\n  mutate(plot_id = \"20\")\n\ncombine_data &lt;- bind_rows(null_data) %&gt;%\n  bind_rows(true_data) %&gt;%\n  mutate(plot_id = fct_shuffle(factor(plot_id)))\n\nggplot(combine_data, aes(x = calories, y = sodium)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(~plot_id) + \n  labs(\n    title = \"Calories vs. Sodium in Fast Food Items\",\n    x = \"Calories\",\n    y = \"Sodium\"\n  )\n\n\n\n\n\n\n\n\nThe plots above represent 19 simulated versions of the fast food data, with one plot showing the real data and relationship between calories and sodium as observed from the TidyTuesday dataset. The others randomly shuffled the sodium values in order to create randomization and remove association between calories and sodium. When reviewing all the plots, plot 20 stands out with a strong positive linear relationship. Plot 20 also corresponds to the real data, providing evidence that there is a true connection in the relationship between calories and sodium. The other simulated plots contain variously scattered data points, ultimately resulting in pretty flat lines with no true relationship or pattern observed. Thus, we have evidence a true statistical relationship between higher calories and more sodium content in fast food items."
  },
  {
    "objectID": "project3.html#end-description",
    "href": "project3.html#end-description",
    "title": "Fast Food",
    "section": "End Description",
    "text": "End Description\nIn this lineup protocol for visual inference simulation, I cleaned the fast food dataset from TidyTuesday in order to observe a potential relationship between calories and sodium. I created a function to create simulated/null datasets, which I then put side by side with a graph of the real data to see if the real data stood out amongst the others. From this, we were able to easily identify the plot of real data as discussed above, providing evidence that there is a strong correlation between higher calories and higher sodium content in fast food items. This finding generally tells us that that fast food items are extremely unhealthy, but worse than we thought - along with having a high calorie content these items will also come with lots of sodium, which means its even worse for you than it would be if just one of the contents was high."
  },
  {
    "objectID": "project3.html#references",
    "href": "project3.html#references",
    "title": "Fast Food",
    "section": "References",
    "text": "References\n“Fast Food Nutrition Facts.” FastFoodNutrition.Org, fastfoodnutrition.org/. Accessed 28 Mar. 2025.\nRfordatascience. “Tidytuesday/Data/2018/2018-09-04 at Main · Rfordatascience/Tidytuesday.” GitHub, github.com/rfordatascience/tidytuesday/tree/main/data/2018/2018-09-04. Accessed 28 Mar. 2025."
  }
]