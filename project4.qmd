---
title: "Exploring Bias in an Amazon Hiring AI"
description: |
  An investigation into Data Science Ethics using an Amazon Hiring AI as an example.
author: Connor Wang
date: April 14, 2025
format: html
---

## The Scenario

Back in 2014 a team of Amazon engineers developed an experimental hiring tool that used artificial intelligence to help sort through job applications. The model was meant to increase efficiency by automatically scoring resumes without the need for a human to spend time reading through them. Candidates were given scores from one to five stars, in order to highlight the most promising candidates for recruiters to look at (Dastin). The team utilized resumes from the past ten years submitted to the company to train the model. The algorithm then learned patterns from this data to predict strong candidates, based on past hirings. Issues arose when the company noticed that the ratings were systematically discriminating against women when they applied for technical jobs (Goodman). Given that the program used past Amazon resumes as blueprints for what an ideal resume looks like, and that males dominated the tech industry at the time, the algorithm learned to favor male applicants. The male candidates were preferred due to various keywords, such as "captured", which were verbs that were more common for male engineers to use. Women candidates were penalized for mentioning the word "women's" in relation to clubs or teams, and discriminated against all-women's colleges (Goodman). As a result of this bias, Amazon stopped using this particular software program and moved on to a recruiting engine that would help with more "rudimentary chores" (Dastin). The overall idea of the algorithm made sense - take in lots of data, and use it to train the model to look out for what Amazon has considered a successful candidate in the past. When doing so, it takes out the human aspect of looking through resumes and getting a feel for the person submitting the job application. The model was just going off the data it was given, which happened to be predominantly male resumes.

## Consent

When using participant data for research or training a model, it is important that the individual consents to their data being used. Since data can include personal information that participants may want to keep private, they must know where their data will be going and how it will be used. They must allow for their data to be used in this way, or state they are fine with the company using their data in any way. In this case, given that resumes from ten years back were used, it is highly unlikely that job applicants signed on to have their resumes included as data when attempting to train this hiring model (Goodman). The individuals submitted the job applications expecting a human recruiter to read and evaluate their resumes, not to be used for a machine learning experiment. Given that Large Language Models and AI were not as prominent in the period when they used these job applications, we can assume that applicants did not give informed consent for their resumes to be used for a model. The applicants are unaware of the context in which their data is being used. Although they were willingly submitting their personal information in the form of job applications and resumes, we do not know if they ever meaningfully said that it was okay to use their data to train the model. Thus, it is difficult to get consent for unforeseen application data.

## Unintended Use

Oftentimes, data can be used in ways that the user did not originally intend. In the example of the Amazon model, data was used that the user submitted for the purpose of getting hired, not to train a hiring model (Dastin). The primary issue with unintended use is that users will submit data for one purpose, but it ends up being used for another. In this case, similar to the aforementioned consent case, the repurposing of this data, which was not originally meant to be used for this model, ended up having unintended consequences. It is necessary to ensure that data being used for a different circumstance than originally intended is valid and can be used in this context. Since the data was just for various Amazon job applications, the model recognized the past hiring patterns, which happened to be in a male-dominated field. The applicants and engineers did not mean for these harms to occur, yet they occurred because the data was used in a way that it was not supposed to be.

## Generalization of the Data

When looking at data, it is important to keep in mind what we are analyzing it for. The data that we are using should be representative of the people who will eventually be affected by the algorithm to make sure it accurately makes predictions for the representative population. Otherwise, we could be using data that is completely unrelated or missing a part of the representative population, and thus creating an algorithm that does not correctly apply. Using a dataset of Amazon's previous tech industry hiring history meant one that consisted of predominantly male resumes (Dastin). This meant training data for the model that was not representative of the population, which in this case was Amazon's applicant pool for tech-related jobs. Based on data that was not generalizable to the whole applicant pool, the model was flawed with gender biases coming from patterns or language that is found in male resumes. The model was going off the data it had, creating a lack of accuracy when it came to gender-diverse applicants. Attempting to make generalizations off of a non-representative dataset can be dangerous when attempting to build models that make big decisions, such as who to hire.

## Recognize and Mitigate Bias

The data values and principles manifesto states that you must try to recognize and mitigate bias in the data that we are using. Given what ultimately happened with the model, it is clear that the Amazon team did not take the steps necessary to mitigate bias. When gathering data, it is important to ask yourself if it could have any bias. Since you have details about the data you are gathering and will have the opportunity to look at it, it is necessary to do a thorough job of making sure no bias can come when training a model. Gathering resumes for a tech-related position in the early 2000s should automatically raise concerns about the groups of applicant data you will be obtaining. It should not have been hard to recognize that gender could play a significant role, and be mitigated by trying to ensure equal amounts of both male and female resumes in the data when training the model. The model failed for one reason - bias. A bias that was easily identifiable if the team stopped and asked themselves if the data they were using could lead to issues such as these. If they categorized resumes by gender, there would likely have been glaring differences between the number of male and female resumes. This was a step that the Amazon team either skipped or did not look into hard enough, and eventually paid the price.

### References

Dastin, Jeffrey. *Insight - Amazon Scraps Secret AI Recruiting Tool That Showed Bias against Women \| Reuters*, www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/. Accessed 4 May 2025.

Goodman, Rachel. “Why Amazon’s Automated Hiring Tool Discriminated against Women: ACLU.” *American Civil Liberties Union*, 27 Feb. 2023, www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against.
