---
title: "Exploring Bias in an Amazon Hiring AI"
description: |
  An investigation into Data Science Ethics using an Amazon Hiring AI as an example.
author: Connor Wang
date: April 14, 2025
format: html
---

## The Scenario

Back in 2014 a team of Amazon engineers developed an experimental hiring tool that used artificial intelligence to help sort through job applications. The model was meant to increase efficiency by automatically scoring resumes without the need for a human to spend the time reading through them. Candidates were given scores from one to five stars, in order to highlight the most promising candidates for recruiters to look at (Reuters). The team utilized resumes from the past ten years submitted to the company to train the model. The algorithm then learned patterns from this data to predict strong candidates, based on past hirings. Issues arose when the company noticed that the ratings were systematically discriminating against women when they applied for technical jobs (ACLU). Given that the program used past Amazon resumes as blueprints for what an ideal resume looks like, and that males dominated the tech industry at the time, the algorithm learned to favor male applicants. The male candidates were preferred due to various key words such as "captured", which were verbs that were more common for male engineers to use. Women candidates were penalized for mentioning the word "women's" in relation to clubs or teams, and discriminated against all-woman colleges (ACLU). As a result of this bias, Amazon stopped using this particular software program and moved onto a recruiting engine that would help with more "rudimentary chores" (Reuters). The overall idea of the algorithm made sense - take in lots of data, and use it to train the model to look out for what Amazon has considered a successful candidate in the past. When doing so, it takes out the human aspect of looking through resumes and getting a feel for the person submitting the job application. The model was just going off the data it was given, which happened to be predominantly male resumes.

## Consent

When using participant data for research or training a model, it is important that they consent to their data being used. Since data can include personal information that participants may want to keep private, it is vital that they know where their data will be going and how it will be used. They must allow for this use of their data, or state they are fine with how the company intends to use the data. In this case, given that resumes from ten years back were used, it is highly unlikely that job applicants signed on to have their resumes included as data when attempting to train this hiring model (ACLU). The individuals submitted the job applications expecting a human recruiter to read and evaluate their resume, not to be used for a machine learning experiment. There was no informed consent from these individuals which raises ethical concerns because the applicants are unaware of the context in which their data is being used. Although they were willingly submitting their personal information in the form of job applications and resumes, we do not know if they ever meaningfully said that it was okay to use their data to train the model. Thus, it is difficult to get consent for applications data that are unforeseen.

## Unintended Use

Oftentimes data can be used in ways that the user did not originally intend for. In the example of the Amazon model, data was used that the user submitted for the purpose of getting hired - not to train a machine learning model (Reuters). The primary issue with unintended use is that users will submit data for one purpose, but it ends up being used in another. In this case, similar to the aforementioned consent case of unforeseen applications, the repurposing of this data that was not originally meant to be used for this model ended up having unintended consequences. It is necessary to ensure that data being used for a different circumstance than originally intended is valid and can be used in this context. Since the data was just for various Amazon job applications, the model recognized the past hiring patterns, which happened to be in a male-dominated field. The applicants and engineers did not mean for these harms, yet they occurred because the data was used in a way that it was not supposed to be.

## Generalization of the Data

When analyzing data it is important to keep in mind what we are analyzing it for. The data that we are using should be representative of the people who we will eventually be using the algorithm on to make sure it accurately makes predictions for the representative population. Otherwise, we could be using data that is completely unrelated or missing a part of the representative population, and thus creating an algorithm that does not correctly apply. Using a dataset of Amazon's previous tech industry hiring history meant one that consisted of predominantly male resumes (Reuters). This created training data for the model that was not representative of the population, which in this case was Amazon's applicant pool for tech related jobs. Based on data that was not generalizable to the whole applicant pool, the model was flawed with gender biases coming from patterns or language that is found in male resumes. The model was going off the data it had, creating a lack of accuracy when it came to gender-diverse applicants. Attempting to make generalizations off of a non-representative dataset can dangerous when attempting to build models that make big decisions, such as who to hire.

## Identifiable Data

When working with personal data it is very important to ensure that people are fine with their data being out there and used in ways that they might not actively know about. An easy way to be free of this ethical concern is to make sure the data is anonymous or old. This way, there are no privacy concerns as there is no way to connect the information in the data to specific people. In the case of the Amazon model, resumes are extremely identifiable documents. Although resumes differ from person to person, it will likely include information such as a full name, address, contact information, work history, and schools. All of these are ways to easily create links to individuals. While it is possible to make resumes somewhat anonymous and remove the direct links to people, there is no indication that Amazon did this with the resumes used to train the hiring algorithm. On top of this, the data being within 10 years does not make it old enough to fight ethical arguments against its use (ACLU). The data was likely stored and saved in development or logs which means a ton of personal data that is not exactly being stored safely. When data can store identifiers to an individual it is hard to protect the identity of the person.

### References

*Insight - Amazon Scraps Secret AI Recruiting Tool That Showed Bias against Women \| Reuters*, www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/. Accessed 15 Apr. 2025.

“Why Amazon’s Automated Hiring Tool Discriminated against Women: ACLU.” *American Civil Liberties Union*, 27 Feb. 2023, www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against.
